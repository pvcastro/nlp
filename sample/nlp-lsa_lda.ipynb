{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 latent methods for dimension reduction and topic modeling\n",
    "\n",
    "![](https://cdn.pixabay.com/photo/2015/11/07/11/17/golden-gate-bridge-1030999_960_720.jpg)\n",
    "Photo: https://pixabay.com/en/golden-gate-bridge-women-back-1030999/\n",
    "\n",
    "Before the state-of-the-art word embedding technique, Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA) area good approaches to deal with NLP problems. Both LSA and LDA have same input which is Bag of words in matrix format. LSA focus on reducing matrix dimension while LDA solves topic modeling problems.\n",
    "\n",
    "I will not go through mathematical detail and as there is lot of great material for that. You may check it from reference. For the sake of keeping it easy to understand, I did not do pre-processing such as stopwords removal. It is critical part when you use LSA, LSI and LDA. After reading this article, you will know:\n",
    "- Latent Semantic Analysis (LSA)\n",
    "- Latent Dirichlet Allocation (LDA)\n",
    "- Take Away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train_raw = fetch_20newsgroups(subset='train')\n",
    "test_raw = fetch_20newsgroups(subset='test')\n",
    "\n",
    "x_train = train_raw.data\n",
    "y_train = train_raw.target\n",
    "\n",
    "x_test = test_raw.data\n",
    "y_test = test_raw.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that words will occurs in similar pieces of text if they have similar meaning. People usually use Latent Semantic Indexing (LSI) as an alternative name in NLP field.\n",
    "\n",
    "First of all, we have m documents and n words as input. An m * n matrix can be constructed while column and row are document and word respectively. You can use count occurrence or TF-IDF score. However, TF-IDF is better than count occurrence in most of the time as high frequency do not account for better classification.\n",
    "\n",
    "![](https://1.bp.blogspot.com/-tnzPA6dDtTU/Vw6EWm_PjCI/AAAAAAABDwI/JatHtUJb4fsce9E-Ns5t02_nakFtGrsugCLcB/s1600/%25E8%259E%25A2%25E5%25B9%2595%25E5%25BF%25AB%25E7%2585%25A7%2B2016-04-14%2B%25E4%25B8%258A%25E5%258D%25881.39.07.png)\n",
    "Photo: http://mropengate.blogspot.com/2016/04/tf-idf-in-r-language.html\n",
    "\n",
    "The idea of TF-IDF is that high frequency may not able to provide much information gain. In another word, rare words contribute more weights to the model. Word importance will be increased if the number of occurrence within same document (i.e. training record). On the other hand, it will be decreased if it occurs in corpus (i.e. other training records). For detail, you may check this [blog](https://towardsdatascience.com/3-basic-approaches-in-bag-of-words-which-are-better-than-word-embeddings-c2cbc7398016).\n",
    "\n",
    "The challenge is that the matrix is very sparse (or high dimension) and noisy (or include lots of low frequency word). So truncated SVD is adopted to reduce dimension.\n",
    "\n",
    "![]()\n",
    "\n",
    "The idea of SVD is finding the most valuable information and using lower dimension t to represent same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from multiprocessing import cpu_count\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW output shape: (11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "def build_bow(x_train, x_test):\n",
    "    tfidf_vec = TfidfVectorizer(use_idf=False, norm='l2')\n",
    "    \n",
    "    transformed_x_train = tfidf_vec.fit_transform(x_train)\n",
    "    transformed_x_test = tfidf_vec.transform(x_test)\n",
    "    \n",
    "    print('BoW output shape:', transformed_x_train.shape)\n",
    "    \n",
    "    return tfidf_vec, transformed_x_train, transformed_x_test\n",
    "\n",
    "tfidf_bow, x_train_bow, x_test_bow = build_bow(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF output shape: (11314, 130107)\n"
     ]
    }
   ],
   "source": [
    "def build_tf_idf(x_train, x_test):\n",
    "    tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
    "    \n",
    "    transformed_x_train = tfidf_vec.fit_transform(x_train)\n",
    "    transformed_x_test = tfidf_vec.transform(x_test)\n",
    "    \n",
    "    print('TF-IDF output shape:', transformed_x_train.shape)\n",
    "    \n",
    "    return tfidf_vec, transformed_x_train, transformed_x_test\n",
    "\n",
    "tfidf_vec, x_train_tfidf, x_test_tfidf = build_tf_idf(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF output shape: (11314, 130107)\n",
      "LSA output shape: (11314, 50)\n",
      "Sum of explained variance ratio: 8%\n",
      "TF-IDF output shape: (11314, 130107)\n",
      "LSA output shape: (11314, 100)\n",
      "Sum of explained variance ratio: 12%\n",
      "TF-IDF output shape: (11314, 130107)\n",
      "LSA output shape: (11314, 200)\n",
      "Sum of explained variance ratio: 19%\n"
     ]
    }
   ],
   "source": [
    "def build_lsa(x_train, x_test, dim=50):\n",
    "    tfidf_vec = TfidfVectorizer(use_idf=True, norm='l2')\n",
    "    svd = TruncatedSVD(n_components=dim)\n",
    "    \n",
    "    transformed_x_train = tfidf_vec.fit_transform(x_train)\n",
    "    transformed_x_test = tfidf_vec.transform(x_test)\n",
    "    \n",
    "    print('TF-IDF output shape:', transformed_x_train.shape)\n",
    "    \n",
    "    x_train_svd = svd.fit_transform(transformed_x_train)\n",
    "    x_test_svd = svd.transform(transformed_x_test)\n",
    "    \n",
    "    print('LSA output shape:', x_train_svd.shape)\n",
    "    \n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Sum of explained variance ratio: %d%%\" % (int(explained_variance * 100)))\n",
    "    \n",
    "    return x_train_svd, x_test_svd\n",
    "\n",
    "x_train_lsa_50, x_test_lsa_50 = build_lsa(x_train, x_test)\n",
    "x_train_lsa_100, x_test_lsa_100 = build_lsa(x_train, x_test, dim=100)\n",
    "x_train_lsa_200, x_test_lsa_200 = build_lsa(x_train, x_test, dim=200)\n",
    "x_train_lsa_400, x_test_lsa_400 = build_lsa(x_train, x_test, dim=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the dimension reduces from 130k to 50, 100, 200 or 400 only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7629 (+/- 0.0256)\n"
     ]
    }
   ],
   "source": [
    "lr_model_bow = LogisticRegression(solver='newton-cg',n_jobs=cpu_count(), multi_class='auto')\n",
    "lr_model_bow.fit(x_train_bow, y_train)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "scores = cross_val_score(lr_model_bow, x_test_bow, y_test, cv=cv, scoring='accuracy')\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8784 (+/- 0.0216)\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(solver='newton-cg',n_jobs=cpu_count(), multi_class='auto')\n",
    "lr_model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    \n",
    "scores = cross_val_score(lr_model, x_test_tfidf, y_test, cv=cv, scoring='accuracy')\n",
    "print(\"Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_lsa_50.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for dim 50: 0.6557 (+/- 0.0369)\n",
      "Accuracy for dim 100: 0.7176 (+/- 0.0301)\n",
      "Accuracy for dim 200: 0.7574 (+/- 0.0371)\n",
      "Accuracy for dim 400: 0.7909 (+/- 0.0284)\n"
     ]
    }
   ],
   "source": [
    "def test_lsa(x_train, x_test):\n",
    "    lr_model = LogisticRegression(solver='newton-cg',n_jobs=cpu_count(), multi_class='auto')\n",
    "    lr_model.fit(x_train, y_train)\n",
    "\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=123)\n",
    "\n",
    "    scores = cross_val_score(lr_model, x_test, y_test, cv=cv, scoring='accuracy')\n",
    "    print(\"Accuracy for dim %s: %0.4f (+/- %0.4f)\" % (x_train.shape[1], scores.mean(), scores.std() * 2))\n",
    "\n",
    "test_lsa(x_train_lsa_50, x_test_lsa_50)\n",
    "test_lsa(x_train_lsa_100, x_test_lsa_100)\n",
    "test_lsa(x_train_lsa_200, x_test_lsa_200)\n",
    "test_lsa(x_train_lsa_400, x_test_lsa_400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take Away\n",
    "- Both of them use __Bag-of-words as input matrix__\n",
    "- The challenge of SVD is that we are __hard to determine the optimal number of dimension__. In general, low dimension consume less resource but we may not able to distinguish opposite meaning words while high dimension overcome it but consuming more resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Me\n",
    "I am Data Scientist in Bay Area. Focusing on state-of-the-art in Data Science, Artificial Intelligence , especially in NLP and platform related. You can reach me from [Medium Blog](https://medium.com/@makcedward) or [Github](https://github.com/makcedward)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- [1] SVD Tutorial: https://cs.fit.edu/~dmitra/SciComp/Resources/singular-value-decomposition-fast-track-tutorial.pdf\n",
    "- [2] CUHK LSI Tutorial: http://www1.se.cuhk.edu.hk/~seem5680/lecture/LSI-Eg.pdf\n",
    "- [3] Stanford LSI Tutorial: https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf\n",
    "- [4] LSA and LDA Explanation: https://cs.stanford.edu/~ppasupat/a9online/1140.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
